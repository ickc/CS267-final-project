<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Kolen Cheung">
  <meta name="author" content="Katherine Oosterbaan">
  <meta name="keywords" content="CS267, Parallel Computing, POLARBEAR, CMB, final project">
  <title>CS 267 Final Project — Application of Parallelization in POLARBEAR’s Pipeline for CMB Analysis through Cython</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="https://ickc.github.io/markdown-latex-css/css/common.css">
  <link rel="stylesheet" href="https://ickc.github.io/markdown-latex-css/fonts/fonts.css">
  <p></p>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">CS 267 Final Project — Application of Parallelization in POLARBEAR’s Pipeline for CMB Analysis through Cython</h1>
<p class="subtitle">Spring 2017, University of California, Berkeley</p>
<p class="author">Kolen Cheung</p>
<p class="author">Katherine Oosterbaan</p>
<p class="date">May  9, 2017</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="#the-physics"><span class="toc-section-number">1.1</span> The Physics</a></li>
<li><a href="#the-pipeline"><span class="toc-section-number">1.2</span> The Pipeline</a></li>
</ul></li>
<li><a href="#optimization-and-parallelization"><span class="toc-section-number">2</span> Optimization and Parallelization</a><ul>
<li><a href="#the-proposal"><span class="toc-section-number">2.1</span> The Proposal</a></li>
</ul></li>
<li><a href="#results-benchmarks-and-discussions"><span class="toc-section-number">3</span> Results, Benchmarks, and Discussions</a><ul>
<li><a href="#ground-template-filter"><span class="toc-section-number">3.1</span> Ground Template Filter</a><ul>
<li><a href="#description"><span class="toc-section-number">3.1.1</span> Description</a></li>
<li><a href="#optimization-procedures"><span class="toc-section-number">3.1.2</span> Optimization Procedures</a></li>
<li><a href="#benchmark"><span class="toc-section-number">3.1.3</span> Benchmark</a></li>
</ul></li>
<li><a href="#boundary-distance-function"><span class="toc-section-number">3.2</span> Boundary Distance Function</a><ul>
<li><a href="#description-1"><span class="toc-section-number">3.2.1</span> Description</a></li>
<li><a href="#optimization-procedures-1"><span class="toc-section-number">3.2.2</span> Optimization Procedures</a></li>
<li><a href="#benchmark-1"><span class="toc-section-number">3.2.3</span> Benchmark</a></li>
</ul></li>
</ul></li>
<li><a href="#conclusion"><span class="toc-section-number">4</span> Conclusion</a></li>
<li><a href="#more-on-boundary-distance-function"><span class="toc-section-number">5</span> More on Boundary Distance Function</a></li>
<li><a href="#polynomial-filter-array"><span class="toc-section-number">6</span> Polynomial Filter Array</a><ul>
<li><a href="#description-2"><span class="toc-section-number">6.1</span> Description</a></li>
<li><a href="#benchmark-2"><span class="toc-section-number">6.2</span> Benchmark</a></li>
</ul></li>
<li><a href="#packaging-and-distributing"><span class="toc-section-number">7</span> Packaging and Distributing</a></li>
<li><a href="#intel-tbb-mpi"><span class="toc-section-number">8</span> Intel TBB, MPI</a></li>
<li><a href="#numba"><span class="toc-section-number">9</span> Numba</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
</nav>
<h1 id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<h2 id="the-physics"><span class="header-section-number">1.1</span> The Physics</h2>
<p>Physicists always push the boundary of our understanding of the most fundamental aspects of the Universe. Some of the fundamental questions we can ask are what constitute the Universe; how gravity plays a role in quantum Physics; why neutrino mass are non-zero, how much mass do they have, and how many numbers of them; and if the current understanding of the Universe through <span class="math inline">\(\Lambda\)</span>-CDM model is correct.</p>
<p>Many of such questions can only be answered when we probed at higher and higher energy scales. For example, the highest energy scale we can achieve artificially in the state-of-the-art LHC is about <span class="math inline">\(\sim \SI{10}{\TeV}\)</span>, or <span class="math inline">\(\SI{10e13}{\eV}\)</span>. But we can do only so much experimentally because of the limit of the size of the equipment we can build, and it is unlikely for the foreseeable future to create energy scale as high as the GUT scale at <span class="math inline">\(\sim \SI{10e16}{\GeV}\)</span>, or <span class="math inline">\(\SI{10e25}{\eV}\)</span>, which will be important for Quantum Gravity.</p>
<p>So instead of relying on human-built machine, one can measure the primordial signals created by the Universe itself. And the oldest possible such signal that is observable is the Cosmic Microwave Background (CMB) Radiation. It is the first light of the Universe when it was <span class="math inline">\(\sim 400000\)</span> years young, and everything happened between now and then are imprinted in this signal. Some of the information we can extracts includes gravitational wave at GUT scale (by B-mode analysis on the CMB), dark matter, neutrino mass, falsification of <span class="math inline">\(\Lambda\)</span>-CDM model, etc.</p>
<p>POLARBEAR is one of the pioneer group on the measurement of CMB polarization in University of California, Berkeley. One of the major result by POLARBEAR in 2014 is<span class="citation" data-cites="Collaboration:2014eg">(Collaboration et al. 2014)</span>:</p>
<blockquote>
<p>the hypothesis of no B-mode polarization power from gravitational lensing is rejected at <span class="math inline">\(97.2\%\)</span> confidence</p>
</blockquote>
<p><embed src="media/CMB-B-mode.pdf" style="width:80.0%" /> </p>
<h2 id="the-pipeline"><span class="header-section-number">1.2</span> The Pipeline</h2>
<p>In order to measure the CMB, a telescope is scanning the sky with <span class="math inline">\(\sim 100\)</span> sensors, each taking a time stream data. An output is created per hour-long observation, called Constant Elevation Scan (CES). There are about <span class="math inline">\(\sim 14\)</span> such observation per day, and after about 2 years of observation, we accumulate about <span class="math inline">\(\sim 10,000\)</span> CES.</p>
<p>These data requires a lot of cleanup, partially due to the imperfection of the equipments, and partially due to the uncontrollable factors like weather and atmospheric conditions. In the end, <span class="math inline">\(4237\)</span> such inputs are used for the analysis.</p>
<p>Since the correlation time scale is <span class="math inline">\(\sim \SI{20}{\min}\)</span>, shorter than the observation time scale, and we can treat each observation independently. Hence, it is a perfectly (embarrassingly) parallel problem.</p>
<p>In the current implementation of AnalysisBackend, the code is mainly written in Python with some occasional C/C++ modules, and relies heavily on external libraries e.g. Numpy. All code is written in serial, and in the end a trivial MPI process is run to start these individual independent processes.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>However, such a perfect parallel treatment is not without its problems:</p>
<ol>
<li><p>Cori’s Haswell nodes has only <span class="math inline">\(128\)</span> GB RAM, this limits the number of concurrent in-node processes to about <span class="math inline">\(16\)</span>, which is significantly less than the <span class="math inline">\(32\)</span> number of cores available.</p></li>
<li><p>Due to the MASTER algorithm <span class="citation" data-cites="Hivon:2001eh">(Hivon et al. 2001)</span>, in the past CMB analysis is mainly <span class="math inline">\(O(n^3)\)</span> which enjoys high computational intensity. The MASTER algorithm improve the scaling to <span class="math inline">\(O(n)\)</span>, but this decreases the computational intensity and therefore the pipeline is highly likely to be IO bounded. It also means that it potentially will be speeded up by hyper-threading.</p></li>
<li><p>Ideally, we would want to run the pipeline using Cori’s Knights Landing (KNL) nodes. This means that it is going to be slightly more RAM-limited, and have much more cores and hyper-threading available. Currently, KNL can afford at most <span class="math inline">\(272\)</span> threads to run concurrently, much bigger than the current <span class="math inline">\(16\)</span> processes per node we are using.</p></li>
<li><p>SIMD vectorization is also important, and is currently unexplored in AnalysisBackend. It is because Haswell CPU has 4-wide and KNL has 8-wide Fused multiply–add (FMA).</p></li>
</ol>
<p>It is worth noting that the Intel Distribution for Python which is also used on NERSC has been optimized for parallelization as well:</p>
<blockquote>
<p>The Intel Distribution accelerates performance of Python packages with Intel® Performance Libraries, including Intel® Math Kernel Library (Intel® MKL), Intel® Threading Building Blocks (Intel® TBB), Intel® Data Analytics Acceleration Library (Intel® DAAL), and Intel® MPI. The packages have been optimized to take advantage of parallelism through the use of threading, multiple nodes, and vectorization. The release notes includes a full list of the packages included in the distribution. From <a href="https://www.continuum.io/sites/default/files/AnacondaIntelFAQFINAL.pdf">Anaconda &amp; Intel Python Distribution FAQ</a>.</p>
</blockquote>
<blockquote>
<p>The all-included, out-of-the box distribution accelerates core Python packages including NumPy, SciPy, pandas, scikit-learn, Jupyter, matplotlib, and mpi4py. It integrates the powerful Intel® Math Kernel Library (Intel® MKL), Intel® Data Analytics Acceleration Library (Intel® DAAL) and pyDAAL, Intel® MPI Library, and Intel® Threading Building Blocks (Intel® TBB). From <a href="https://software.intel.com/en-us/intel-distribution-for-python#close">Intel® Distribution for Python* | Overview | Intel® Software</a>.</p>
</blockquote>
<p>Since most of the existing code heavily relies on Numpy, and from the description above one would expect free parallelization from Intel’s Distribution for Python. This turn out not to be the case at least in our study. See more in the <a href="#polynomial-filter-array">benchmark of Polynomial Filter Array</a> in the Appendix. More investigation is needed to see what kind of parallelization can be expected from the libraries in Intel’s Distribution for Python.</p>
<h1 id="optimization-and-parallelization"><span class="header-section-number">2</span> Optimization and Parallelization</h1>
<p>From the above limitation, we laid out the following requirements:</p>
<ol>
<li><p>SIMD vectorization and OpenMP parallelization is necessary, for both Cori’s Haswell and KNL nodes, and more so on the later one.</p></li>
<li><p>(non-trivial) MPI<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> is not necessary however, because each CES is still independent, and in the foreseeable future the RAM needed for a single CES will not surpass the RAM available in 1 node on Cori.</p></li>
<li><p>Written in Python. This has been mostly true in AnalysisBackend. The choice of Python can be attributed to historical reasons, but also because of maintainability (e.g. in writing, packaging, distributing, testing) and readability.</p></li>
</ol>
<h2 id="the-proposal"><span class="header-section-number">2.1</span> The Proposal</h2>
<p>Our proposal is then to write every module in Cython.</p>
<p>Cython as a language is a superset of Python, and as a compiler is a transpiler to C/C++. It supports both SIMD vectorization and OpenMP parallelism. Many but not all C/C++ features are supported.</p>
<p>Cython is not without its limitation. Directive pragma and Intel intrinsics are not supported, SIMD vectorization relies on the C/C++ compilers’ optimization, and obtaining the vectorization report is convoluted<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>. OpenMP support is limited, e.g. <code>#pragma omp for</code> can be used in Cython by <code>prange</code>, reduction is implied by in-place operators, and as a result other kinds of reduction like <code>max</code> are not supported.</p>
<p>Fortunately, these limitations will not be important to POLARBEAR’s analysis, as shown in the demonstration below. One of the reason is that there are a lot of inherit parallelism in the pipeline. For example, each channel from the data are usually independently processed, which is a trivial use case of <code>prange</code>.</p>
<h1 id="results-benchmarks-and-discussions"><span class="header-section-number">3</span> Results, Benchmarks, and Discussions</h1>
<p>As a proposal on parallelizing the whole pipeline, we focuses on 3 functions in the pipeline to demonstrate different aspects of the difficulties involved and the potential speed gain from it. They are (more details will be given below):</p>
<ol>
<li><p>Ground template filter: was originally written in Python with Numpy and C with Weave<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. This is a testbed to see how to fully Cythonize a Python function, i.e. written in C-style with Cython syntax without the use of Numpy at all.</p></li>
<li><p>Polynomial filter: was originally written in pure Python with Numpy. This is a testbed on the potential speed gain on a function that is highly pythonic and heavily relies on Numpy that full Cythonization is virtually impossible (without reinventing the wheels from Numpy or using similar libraries in C/C++).</p></li>
<li><p>Boundary distance function: was originally written in Python with Numpy and C with weave. This is one of the major bottleneck of the whole pipeline, and is the testbed to see how much speed up we could get from Cythonization in an actual application hotspot.</p></li>
</ol>
<p>Ground template filter and boundary distance function are both rewritten completely Cythonic<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> without the using of Numpy functions<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>. For the polynomial filter, it is minimally Cythonized because of the heavy reliance on Numpy’s functions. Since there is virtually no speed gain from the OpenMP parallelization in the polynomial filter, it will be mentioned in the appendix. In the following 2 sections, we will focus on the ground template filter and boundary distance function. All the codes can be found in <a href="https://github.com/ickc/TAIL">GitHub - ickc/TAIL</a>.</p>
<p>The following benchmarks are done on Cori’s Haswell nodes. Although we want to use the KNL nodes to push the boundary of vectorization and OpenMP scaling, Cori’s KNL queue has been impenetrable on the days near the submission of this report. In the near future we will profile it on KNL.</p>
<h2 id="ground-template-filter"><span class="header-section-number">3.1</span> Ground Template Filter</h2>
<p>The actual code for this filter can be found in <a href="https://github.com/ickc/TAIL/blob/master/tail/timestream/ground_template_filter_array.pyx">ickc/TAIL/ground_template_filter_array.pyx · GitHub</a>.</p>
<h3 id="description"><span class="header-section-number">3.1.1</span> Description</h3>
<p>First, we scan the sky in constant elevation with a sweeping azimuth:</p>
<p><embed src="media/az.pdf" style="width:55.0%" /> </p>
<p>And there is a boolean mask that select the data and discard poor data:</p>
<p><embed src="media/mask.pdf" style="width:55.0%" /> </p>
<p>And finally the signal:</p>
<p><embed src="media/signal.pdf" style="width:55.0%" /> </p>
<p>The mask is for data selection. So the main focus here are the signal <span class="math inline">\(d(t)\)</span> and the azimuth <span class="math inline">\(\theta(t)\)</span>. These 2 equations can be considered as a set of parametric equations in time <span class="math inline">\(t\)</span>. And the ground template filter is basically to find the <span class="math inline">\(\langle d(\theta) \rangle\)</span>, the average signal as a function of <span class="math inline">\(\theta\)</span>.</p>
<p>In the algorithm, this is done through a given resolution <span class="math inline">\(n\)</span> (say, <span class="math inline">\(300\)</span> pixels), the range in <span class="math inline">\(\theta\)</span> is divided into <span class="math inline">\(n\)</span> bins, and the signal is averaged into these bins.</p>
<h3 id="optimization-procedures"><span class="header-section-number">3.1.2</span> Optimization Procedures</h3>
<h4 id="vectorization"><span class="header-section-number">3.1.2.1</span> Vectorization</h4>
<p>The first step is to look at the HTML report which can be found in <a href="https://ickc.github.io/TAIL/tail/timestream/ground_template_filter_array.html">Cython: ground_template_filter_array.pyx</a>, and below we have an excerpt of it:</p>
<p><embed src="media/cythonization-report.pdf" style="width:60.0%" /> </p>
<p>The key to look for in these reports are the “yellow-ness”. More yellow means more Python interactions, and if it is white as in above, it is transpiled into pure C/C++ and Python’s Global Interpreter Lock (GIL) can be released, which is important for OpenMP parallelization.</p>
<p>The next step is to check for vectorization and see if there is anyway to rewrite it to trigger vectorization. For example, from the same code above, if you click at the <code>+</code> sign in line 65 in the HTML version, you can see the C/C++ code it is translated into.</p>
<p>From <a href="https://ickc.github.io/TAIL/tail/timestream/ground_template_filter_array.cpp"><code>ickc/TAIL/tail/timestream/ground_template_filter_array.cpp</code></a>, we can then find where this piece of C/C++ code is in the generated <code>.cpp</code> file. Trace it back to the nearest for-loop, which is in line 2303.</p>
<p>Finally, you can go into the <a href="https://ickc.github.io/TAIL/build/temp.macosx-10.6-x86_64-3.5/tail/timestream/ground_template_filter_array.optrpt">Intel vectorization report <code>ground_template_filter_array.optrpt</code></a> and find the reference of line 2303, and it reads</p>
<blockquote>
<pre><code>remark #15344: loop was not vectorized: vector dependence prevents vectorization</code></pre>
</blockquote>
<p>In this case, we will replace the code from line 63-66 by 69-71. Repeating the whole exercise and the report will say</p>
<blockquote>
<pre><code>loop was not vectorized: vectorization possible but seems inefficient.
Use vector always directive or -vec-threshold0 to override </code></pre>
</blockquote>
<p>In this instance, rewriting the code to remove the branching statement made vectorization possible, but still inefficient and the compiler didn’t vectorize it. It is worth noting that the code in line 69-71 is slower than the one in line 63-66. Without checking the report, one would write as in line 69-71 to hope for vectorization but actually get a slower code.</p>
<p>In other instances of the code, such kind of optimization paid off. One such case is to use the Conditional (ternary) Operator to rewrite an if-statement.</p>
<p>Another interesting instance in this particular problem is</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(size):
    bins_hit[i] <span class="op">=</span> EPSILON
...
<span class="co">## SIMD checked</span>
<span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(nPix):
    <span class="co"># won&#39;t be 0 since it is initialized as EPSILON</span>
    <span class="co"># if bins_hit[nBin * i + k] != 0:</span>
    bins_signal[nBin <span class="op">*</span> i <span class="op">+</span> k] <span class="op">/=</span> bins_hit[nBin <span class="op">*</span> i <span class="op">+</span> k]</code></pre></div>
<p>In order to remove the special case of dividing by <span class="math inline">\(0\)</span>, <code>bins_hit</code> is declared to be <code>double</code> instead of <code>int</code>, and initialized to be the machine epsilon rather than <span class="math inline">\(0\)</span>. In the worst case scenario it makes an error of the order <span class="math inline">\(10^{-16}\)</span>, but is going to speed up this part of the code a lot through SIMD vectorization. Since we know that in our application such a small error is acceptable, this will be a good trade off for us.</p>
<p>Of all the vectorization effort, one feature we missed is the ability to control the alignment of the memory. Numpy array are created in <span class="math inline">\(16\)</span>-bytes alignment, which will be provided as the input of the function. But in order to fully optimized for vectorization, <span class="math inline">\(64\)</span>-bytes alignment is needed. Even worst, even if it is <span class="math inline">\(64\)</span>-bytes aligned<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>), compiler hints through directive pragma is not support in Cython and therefore no speed up can be done by alignment.</p>
<h4 id="locality"><span class="header-section-number">3.1.2.2</span> Locality</h4>
<p>In this example, it is also worth mentioning locality is also important. It is because the computation is of low intensity, the problem is IO bound and we want to minimize the time spent in IO.</p>
<p>From the description above, we see that the <span class="math inline">\(\theta (t)\)</span> is sweeping back and forth, and the signal <span class="math inline">\(d(t)\)</span> is averaged per bin in <span class="math inline">\(\theta\)</span>. i.e. as it is swept through, either you have the locality of <span class="math inline">\(d(t)\)</span>, or the locality of the bins of <span class="math inline">\(\theta\)</span>, but not both.</p>
<p>Furthermore, this function has a key <code>lr</code>, when true, the averaged signaled is calculated separately when <span class="math inline">\(\theta\)</span> is sweeping left or right. In the original version of the code, this is done in 2 passes, where each pass has a mask to remove the signal that move in another direction (e.g. for left pass, right moving <span class="math inline">\(\theta\)</span> are removed by a mask). This is going to be RAM inefficient and slow because the IO on the signal is twice as long and require a mask as large as the original signal.</p>
<p>When such conflicts exists, the locality is prioritized towards the signal rather than the bins. Because the total number of bins is much less than that of the signals, so the bins has a higher chance to stay in a higher level of cache (say L3), whereas the signal is typically larger than L3 cache. One such example is this code:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="cf">for</span> i <span class="kw">in</span> prange(nCh, nogil<span class="op">=</span><span class="va">True</span>, schedule<span class="op">=</span><span class="st">&#39;guided&#39;</span>, num_threads<span class="op">=</span>num_threads):
    <span class="co"># calculate ground template</span>
    <span class="co">## add total signal and no. of hits</span>
    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(nTime):
        <span class="cf">if</span> mask[i, j]:
            k <span class="op">=</span> pointing[j]
            <span class="cf">if</span> isMovingRight[j]:
                bins_signal_r[nBin <span class="op">*</span> i <span class="op">+</span> k] <span class="op">+=</span> input_array[i, j]
                bins_hit_r[nBin <span class="op">*</span> i <span class="op">+</span> k] <span class="op">+=</span> <span class="dv">1</span>
            <span class="cf">else</span>:
                bins_signal_l[nBin <span class="op">*</span> i <span class="op">+</span> k] <span class="op">+=</span> input_array[i, j]
                bins_hit_l[nBin <span class="op">*</span> i <span class="op">+</span> k] <span class="op">+=</span> <span class="dv">1</span></code></pre></div>
<p>In the if-statement above, priority of locality is given to the signal <code>input_array</code>. If we wanted to favor the locality of the bins, the 2 branches should be split into their own for-loop.</p>
<h4 id="openmp"><span class="header-section-number">3.1.2.3</span> OpenMP</h4>
<p>The same example above also shows the use of OpenMP. The <code>nogil</code> is important here. Only region with no “yellow-ness” in the Cython to C/C++ conversion can releases the GIL. So if one wants to use OpenMP parallelization, then the content within the loop needs to be free of Python interaction.</p>
<p>This example also shows that the <code>i</code>-loop that runs through the <code>nCh</code>, stands for the number of channels, are independent. This is often true in the filters and will be trivial to parallelized.</p>
<h3 id="benchmark"><span class="header-section-number">3.1.3</span> Benchmark</h3>
<p>With number of channels <span class="math inline">\(100\)</span>, number of time-stream signal <span class="math inline">\(10000\)</span>, and number of bins <span class="math inline">\(300\)</span>, <code>lr=True</code>, the ground template filter in the old pipeline takes <span class="math inline">\(\SI{7.53}{\ms}\)</span> to complete, and our code takes <span class="math inline">\(\SI{1.96}{\ms}\)</span> to complete. i.e. a speed up of <span class="math inline">\(3.84\)</span> times, just from Cythonization and SIMD <em>without</em> OpenMP parallelization yet.</p>
<p>As we add more numbers of concurrent threads, the scaling looks like the following, using number of channels <span class="math inline">\(50,000\)</span>, number of time-stream signal <span class="math inline">\(10,000\)</span>, and number of bins <span class="math inline">\(8,192\)</span>, <code>lr=True</code>:</p>
<p><embed src="media/ground_template.pdf" style="width:85.0%" /> </p>
<p>It can be seen that this does not scale very well beyond <span class="math inline">\(8\)</span> threads, and hyper-threading actually will make it slower. The reason is that even for this unrealistically large no. of channels, the computation intensity is still very low and the OpenMP overhead is relatively high when <span class="math inline">\(p\)</span> is large — it is likely that all threads are accessing the L3 cache where the bins mostly lived and the RAM where the signal mostly lived and are congressed. i.e. in the roofline model we are on the way left side of the roof.</p>
<p>Fortunately, since we are starting <span class="math inline">\(16\)</span> processes. When Haswell/KNL nodes are used, the number of threads is <span class="math inline">\(2\)</span>/<span class="math inline">\(4\)</span> respectively (if hyperthreading is not used as it is shown to be not helping). In this region,</p>
<p><embed src="media/ground_template-2.pdf" style="width:85.0%" /> </p>
<p>we see that the strong scaling coefficient is <span class="math inline">\(0.406\)</span>, meaning that the time needed scales as <span class="math inline">\(\frac{1}{p^{0.406}}\)</span>. As mentioned above, the low coefficient is explained by the low computational intensity.</p>
<h2 id="boundary-distance-function"><span class="header-section-number">3.2</span> Boundary Distance Function</h2>
<p>The actual code for this filter can be found in <a href="https://github.com/ickc/TAIL/blob/master/tail/timestream/boundary_distance.pyx">ickc/TAIL/boundary_distance.pyx · GitHub</a>.</p>
<h3 id="description-1"><span class="header-section-number">3.2.1</span> Description</h3>
<p>This function is much easier to be described, involving well-known algorithm in Computer Science: boundary tracing algorithm and post-office problem.</p>
<p>The function takes a <span class="math inline">\(n\times n\)</span> 2D array, which has 1 simply-connected region of <code>True</code> value, and elsewhere <code>False</code>. It can be visualized as 1 single island of <code>True</code> value around a sea of <code>False</code> in a square map<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>.</p>
<p>What this function does is for every point in the island, find the distance to the closest point on the boundary. A 2D array of these distances are returned as a Numpy array.</p>
<p>We implemented it by first using a Boundary tracing algorithm to detect the boundary first. After that, it is a post-office problem — for every point inside the boundary, determine the closest point on the boundary. Hence this is an <span class="math inline">\(O(n^3)\)</span> problem.</p>
<h3 id="optimization-procedures-1"><span class="header-section-number">3.2.2</span> Optimization Procedures</h3>
<p>The vectorization and OpenMP parallelization procedure is the same as that laid out in <a href="#ground-template-filter">Ground Template Filter</a> and we are not repeating it here. The Cythonization report is in <a href="https://ickc.github.io/TAIL/tail/timestream/boundary_distance.html">Cython: boundary_distance.pyx</a> (and you can see most Python interaction is avoided), and the generated <code>.cpp</code> code is in <a href="https://ickc.github.io/TAIL/tail/timestream/boundary_distance.cpp">ickc/TAIL/tail/timestream/boundary_distance.cpp</a>, and the <a href="https://ickc.github.io/TAIL/build/temp.macosx-10.6-x86_64-3.5/tail/timestream/boundary_distance.optrpt">vectorization report is in here</a>.</p>
<p>One example that shows both SIMD vectorization and OpenMP parallelization is</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="cf">for</span> i <span class="kw">in</span> prange(x_min <span class="op">+</span> <span class="dv">1</span>, x_max, nogil<span class="op">=</span><span class="va">True</span>, schedule<span class="op">=</span><span class="st">&#39;guided&#39;</span>, num_threads<span class="op">=</span>num_threads):
    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(y_min <span class="op">+</span> <span class="dv">1</span>, y_max):
        <span class="cf">if</span> mask[i, j]:
            loc <span class="op">=</span> i <span class="op">*</span> m <span class="op">+</span> j
            <span class="co"># SIMD checked</span>
            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(nBoundary):
                distance_sq <span class="op">=</span> (i <span class="op">-</span> boundary_coordinate[<span class="dv">2</span> <span class="op">*</span> k])<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="op">\</span>
                    (j <span class="op">-</span> boundary_coordinate[<span class="dv">2</span> <span class="op">*</span> k <span class="op">+</span> <span class="dv">1</span>])<span class="op">**</span><span class="dv">2</span>
                distances_sq[loc] <span class="op">=</span> distance_sq <span class="op">\</span>
                    <span class="cf">if</span> distance_sq <span class="op">&lt;</span> distances_sq[loc] <span class="cf">else</span> distances_sq[loc]</code></pre></div>
<p>This is the post office problem, where it is determining the distances of each points to the boundary, and is the part that the function is going to spend the majority of time in because it is <span class="math inline">\(O(n^3)\)</span>. In the innermost <code>k</code>-loop, it is vectorized using the ternary conditional operator, without which the if-statement would have prevented vectorization. In the outermost <code>i</code>-loop, OpenMP-for is used, where <code>i</code> represent the <code>i</code>-th row of the pixels on the map, which are independent of each other.</p>
<p>Another (counter) example is</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># convert boundary from 1D indexing to 2D indexing</span>
<span class="co"># and obtain the smallest box that includes the boundary</span>
<span class="co"># Not vectorized, use the following if in C</span>
<span class="co">#pragma ivdep</span>
<span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(nBoundary):
    x <span class="op">=</span> boundary[k] <span class="op">//</span> m
    y <span class="op">=</span> boundary[k] <span class="op">%</span> m
    boundary_coordinate[<span class="dv">2</span> <span class="op">*</span> k] <span class="op">=</span> x
    boundary_coordinate[<span class="dv">2</span> <span class="op">*</span> k <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> y
    x_min <span class="op">=</span> x <span class="cf">if</span> x <span class="op">&lt;</span> x_min <span class="cf">else</span> x_min
    y_min <span class="op">=</span> y <span class="cf">if</span> y <span class="op">&lt;</span> y_min <span class="cf">else</span> y_min
    x_max <span class="op">=</span> x <span class="cf">if</span> x <span class="op">&gt;</span> x_max <span class="cf">else</span> x_max
    y_max <span class="op">=</span> y <span class="cf">if</span> y <span class="op">&gt;</span> y_max <span class="cf">else</span> y_max</code></pre></div>
<p>Because of the apparent vector-dependance, it is not vectorized. If it were written in C/C++, we would have used a <code>#pragma ivdep</code> at this point. Fortunately it is <span class="math inline">\(O(n)\)</span> and is not the hotspot of the application.</p>
<p>It is worth mentioning that the boundary tracing algorithm is not parallelized nor anywhere has SIMD vectorization in it. This part of the code is essentially having a class of object <code>Turtle</code> that walks down the map and keep tracing the boundary until it goes back to the starting point.</p>
<p>Vectorization is impossible in the boundary tracing algorithm, but OpenMP parallelization is at least in theory possible. We could have divided the map into <span class="math inline">\(p\)</span> regions and starts “workers” to trace the boundaries in parallel and later merge them together. However, it will be challenging to merge the individual boundaries together to make it becomes one big boundary, while remains robust.</p>
<p>The reason we choose not to parallelize this region is because this algorithm is <span class="math inline">\(O(n)\)</span> (where the input mask is of size <span class="math inline">\(n\times n\)</span>), and since it is not optimizable, potential speed-up from even KNL is at most <span class="math inline">\(68\)</span> times (and in reality very much less because we are starting <span class="math inline">\(16\)</span> processes at once), without even account for the time to combine all those boundaries.</p>
<p>However, we need to stress that this possibility is not ruled out. If in the future this becomes a hotspot, such parallelization scheme should be investigated. In fact, although we wrote this part of the code using object-oriented programming, we used Cython’s Extension types such that they are not really Python classes but are translated into structs and functions. i.e. All Python interaction is removed and GIL can be released, which is the cornerstone of using OpenMP in Cython. A bonus of this design is that there is no Python overhead and the algorithm is very fast.</p>
<h3 id="benchmark-1"><span class="header-section-number">3.2.3</span> Benchmark</h3>
<p>We created a mask of <span class="math inline">\(1024 \times 1024\)</span> large, and an island of radius <span class="math inline">\(128\)</span> in the middle. We run the benchmark on the original AnalysisBackend algorithm at <span class="math inline">\(\SI{583}{ms}\)</span>, and our algorithm at <span class="math inline">\(\SI{119}{ms}\)</span>, i.e. a speed up of <span class="math inline">\(4.90\)</span> times, just from Cythonization and SIMD <em>without</em> OpenMP parallelization yet.</p>
<p>In the strong scaling test, we changed the radius to <span class="math inline">\(510\)</span>, and the result is</p>
<p><embed src="media/boundary_distance.pdf" style="width:85.0%" /> </p>
<p>we see that it is almost perfectly parallelized with a strong scaling coefficient of <span class="math inline">\(0.909\)</span>, meaning that the time needed scales as <span class="math inline">\(\frac{1}{p^{0.909}}\)</span>. This is close to ideal (<span class="math inline">\(1\)</span>), and is expected because the rows of pixels are totally independent and hence is perfectly parallel.</p>
<h1 id="conclusion"><span class="header-section-number">4</span> Conclusion</h1>
<p>Speedup is achieved through</p>
<ol>
<li>Cythonization</li>
<li>SIMD vectorization</li>
<li>OpenMP parallelization</li>
</ol>
<p>It is hard to decouple the 1st and 2nd effect. For the combined effect of Cythonization and SIMD vectorization, in the worst case scenario which involved minimal changes to the Python code hence remains Pythonic, the <a href="#polynomial-filter-array">Polynomial Filter Array</a> in the appendix, has <span class="math inline">\(3.23\)</span> speedup. In the best case scenario where Python interaction is minimized without using Numpy functions, the <a href="#boundary-distance-function">Boundary Distance Function</a>, a speedup of <span class="math inline">\(4.90\)</span> is achieved.</p>
<p>For the 3rd effect, <a href="#polynomial-filter-array">Polynomial Filter Array</a> which stays Pythonic and relies on Numpy has virtually <span class="math inline">\(0\)</span> efficiency. More investigation is needed here, as mentioned in the appendix. For the <a href="#ground-template-filter">Ground Template Filter</a>, the strong scaling coefficient is <span class="math inline">\(0.406\)</span> because of low computational intensity, and fortunately is not the hotspot of the application<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> Lastly, in the <a href="#boundary-distance-function">Boundary Distance Function</a>, high strong scaling coefficient is achieved at <span class="math inline">\(0.909\)</span>. This will be important since this is a hotspot for the application.</p>
<p>To give a very rough estimation on the potential speedup for the whole pipeline, we will give a factor of <span class="math inline">\(4\)</span> for Cythonization and SIMD vectorization. We guesstimate the strong scaling to be ~<span class="math inline">\(0.5\)</span>, with a huge grain of salt that it varies a lot through these 3 different functions. From this preliminary study, hyper-threading does not help and so we are not considering this. Since we use <span class="math inline">\(16\)</span> process per node, for Cori’s Haswell nodes, we would use <span class="math inline">\(2\)</span> threads per process, and for KNL, we would use <span class="math inline">\(4\)</span> threads per process. The overall estimated factor of speed up will then be:</p>
<ul>
<li>Haswell nodes: <span class="math inline">\(\sim 6\)</span></li>
<li>KNL nodes: <span class="math inline">\(\sim 8\)</span></li>
</ul>
<p>For a job that would have required <span class="math inline">\(\sim 220,000\)</span> NERSC hours on Haswell, it would becomes <span class="math inline">\(\sim 40,000\)</span>. It should be emphasized that this is a very rough estimation though, and more detailed analysis is needed in the future.</p>
<p></p>
<h1 id="more-on-boundary-distance-function"><span class="header-section-number">5</span> More on Boundary Distance Function</h1>
<p>The boundary distance function is actually part of a larger code called the pseudo-power spectrum. Boundary distance function is needed for an apodization mask, which essentially filter the map such that the “island” has a smooth transition to zero on the boundary.</p>
<p>This apodization mask might be created by an <span class="math inline">\(O(n)\)</span> algorithm instead (whereas the boundary distance function is <span class="math inline">\(O(n^3)\)</span>), essentially by walking through the boundary <span class="math inline">\(k\)</span>-times, where <span class="math inline">\(k\)</span> is the width of the apodization mask.</p>
<p>This has not been done in this study, because we find it very hard to parallelized a boundary tracing algorithm. One suggestion is made in <a href="#boundary-distance-function">Boundary Distance Function</a> and might be investigated in the future.</p>
<p>However, this alternative proposal on the apodization mask might turns out to be more effective, even when run in serial, because as we are making higher and higher resolution map, <span class="math inline">\(O(n)\)</span> is always going to win the <span class="math inline">\(O(n^3)\)</span> algorithm.</p>
<p>More investigation is needed in this comparison. And in fact a quick demo based on the implementation of boundary distance function in this report is possible. Because a boundary tracing algorithm is already implemented in this study, and the proposed apodization mask is basically an iteration of the boundary tracing <span class="math inline">\(k\)</span> times.</p>
<h1 id="polynomial-filter-array"><span class="header-section-number">6</span> Polynomial Filter Array</h1>
<h2 id="description-2"><span class="header-section-number">6.1</span> Description</h2>
<p>The poly_filter_array code uses a Legendre polynomial to fix the gain of the data, as the gain drifts during the scanning period. Unlike ground_template_filter_array , poly_filter_array relies heavily on Numpy functions, which for pure Python code provides good speedup. However, it is very unfortunate when one is trying to transition the code to Cython.</p>
<p>To transition the code to Cython, we rewrote the code in the Cython memoryview array style, similar to the ground template filter. However, when it came time to parallelize, we ran into serious problems. To parallelize Cython code, it is required that we release the GIL, which is not possible when code contains Numpy calls. Unfortunately, this was almost the entire code, which meant that the few cases where we actually could call prange resulted in no speedup whatsoever. For a future direction, we recommend rewriting the code to eliminate the use of Numpy, which would enable the code to be reformatted in C-style and parallelized, which is the maximum Cython speedup.</p>
<p>One more point of interest with this code is that it was written and compiled in a Jupyter notebook that was run within NERSC. NERSC uses the Intel MKL distribution of Python and Numpy, which actually already uses OpenMP to deliver thread-level parallelism and include specialized vectorization instructions. This means that simply by compiling and running the code on NERSC, we may already be taking advantage of parallelism that’s implicitly built into the Python framework.</p>
<p>In summary, when code contains significant amounts of Numpy operations, optimal conversion to Cython and Cythonized parallelization is not possible. To fully implement code in Cython and to parallelize it, code must be rewritten to eliminate Numpy calls.</p>
<h2 id="benchmark-2"><span class="header-section-number">6.2</span> Benchmark</h2>
<p>For number of channels <span class="math inline">\(10,000\)</span>, number of time stream data point <span class="math inline">\(10,000\)</span>, and polynomial order <span class="math inline">\(4\)</span>:</p>
<p>In the original code, it took <span class="math inline">\(\SI{1.14}{\s}\)</span> and in the cythonized code, <span class="math inline">\(\SI{353}{\ms}\)</span> in serial. i.e. <span class="math inline">\(3.23\)</span> times improvements.</p>
<p>However, there is virtually no difference in timing when number of threads is increased. This is unexpected because as mentioned in <a href="#the-pipeline">The Pipeline</a>, parallelization should be built-in from the Numpy provided by Intel’s Distribution for Python.</p>
<p>More investigation is needed here in the future, and perhaps requires a deeper understanding of what kind of parallelism is provided in Intel’s Distribution of Python.</p>
<p>Because there is no parallelization scaling in this function (the coefficient is virtually <span class="math inline">\(0\)</span>), we decided not to put this part in the main report but in the appendix.</p>
<h1 id="packaging-and-distributing"><span class="header-section-number">7</span> Packaging and Distributing</h1>
<p>Our modules is built using <code>setup.py</code>. All the compiler args can be put in <code>setup.py</code>, including the <code>-O</code> flag levels, <code>-mtune</code>, <code>-march</code> flags, etc. More investigation is needed on how to target different compilers at the same time. For instance, Clang compiler does not recognize the <code>-fopenmp</code> flag.</p>
<p>Currently, Intel’s <code>icpc</code> compiler and GNU’s <code>g++</code> compiler is supported. We found that <code>g++</code> compiler does not scale very well for OpenMP when there are too much threads than the number of jobs in <code>prange</code>. And for our codes, generally the <code>icpc</code> compiler does better at optimizing for speed.</p>
<p>Packaging is going to be important for our pipeline. In fact one of the forte of Python is packaging and distributing of the code. In the future we should investigate on using Conda build recipes to improve packaging and distribution, and also docker and shifter for both ease of deployment and potential speedup<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>. One potential challenge here might be the use of <code>icpc</code> compiler.</p>
<p>We should also mentioned that docstrings and comments are emphasized in TAIL, which is lacking in AnalysisBackend. In the future, these docstrings can easily be turned into documentation through a very popular Python library called Sphinx. We strongly believed that good documentation should play an important role in distributing our code.</p>
<p>Continuous Integration is also employed using <a href="https://travis-ci.org/ickc/TAIL">Travis</a>. Python 2.7 and Python 3.6 are test against, and the Cythonized C++ code is built using GNU’s <code>g++</code> compiler. Intel’s <code>icpc</code> compiler for Continuous Integration is possible[<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>(<a href="https://github.com/nemequ/icc-travis" class="uri">https://github.com/nemequ/icc-travis</a>)] but has been unreliable to the author and not used. However, every build is tested locally using Intel’s compiler before pushing to guarantee the code is always in a working state.</p>
<h1 id="intel-tbb-mpi"><span class="header-section-number">8</span> Intel TBB, MPI</h1>
<p>In the situations that involve more complicated parallelization, MPI and Intel Threading Building Blocks (TBB) might be beneficial. We did not try either approach here, but Intel has some nice articles on using MPI and TBB in Python:</p>
<ul>
<li><a href="https://software.intel.com/en-us/blogs/2016/04/04/unleash-parallel-performance-of-python-programs">Unleash the Parallel Performance of Python* Programs | Intel® Software</a></li>
<li><a href="https://software.intel.com/en-us/articles/exploring-mpi-for-python-on-intel-xeon-phi-processor">Exploring MPI for Python* on Intel® Xeon Phi™ Processor | Intel® Software</a></li>
</ul>
<p>In particular, Intel TBB can becomes important if different levels of parallelization is applied where over-subscription might arise. Currently, all 3 functions involved in this study has incorporate a key-value function argument of <code>numthreads</code> default to be 4. If function composition is used, these <code>numthreads</code> should be set properly to avoid oversubscription. One possible alternative will be setting <code>numthreads</code> with the environment variables <code>OMP_NUM_THREADS</code> and uses Intel TBB to get around with oversubscription whenever it arises.</p>
<h1 id="numba"><span class="header-section-number">9</span> Numba</h1>
<p>One of the options that we pursued for code speedup/parallelization was Numba. Numba is a module that generates optimized machine code from a python codebase using the LLVM compiler structure. LLVM is a single static assignment(SSA)-based compilation structure. It allows you to just-in-time compile Python code (including Numpy) at import time, runtime, or statically. To test the speedup, we ran a nontrivial test function; the Numba additions are in red (jit is just-in-time compiling).</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">from</span> numba <span class="im">import</span> jit

<span class="at">@jit</span>
<span class="kw">def</span> naive_convolve(f, g):
    vmax <span class="op">=</span> f.shape[<span class="dv">0</span>]
    wmax <span class="op">=</span> f.shape[<span class="dv">1</span>]
    smax <span class="op">=</span> g.shape[<span class="dv">0</span>]
    tmax <span class="op">=</span> g.shape[<span class="dv">1</span>]
    smid <span class="op">=</span> smax <span class="op">//</span> <span class="dv">2</span>
    tmid <span class="op">=</span> tmax <span class="op">//</span> <span class="dv">2</span>
    xmax <span class="op">=</span> vmax <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>smid
    ymax <span class="op">=</span> wmax <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>tmid
    h <span class="op">=</span> np.zeros([xmax, ymax], dtype<span class="op">=</span>f.dtype)
    <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(xmax):
        <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(ymax):
            s_from <span class="op">=</span> <span class="bu">max</span>(smid <span class="op">-</span> x, <span class="op">-</span>smid)
            s_to <span class="op">=</span> <span class="bu">min</span>((xmax <span class="op">-</span> x) <span class="op">-</span> smid, smid <span class="op">+</span> <span class="dv">1</span>)
            t_from <span class="op">=</span> <span class="bu">max</span>(tmid <span class="op">-</span> y, <span class="op">-</span>tmid)
            t_to <span class="op">=</span> <span class="bu">min</span>((ymax <span class="op">-</span> y) <span class="op">-</span> tmid, tmid <span class="op">+</span> <span class="dv">1</span>)
            value <span class="op">=</span> <span class="dv">0</span>
            <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(s_from, s_to):
                <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(t_from, t_to):
                    v <span class="op">=</span> x <span class="op">-</span> smid <span class="op">+</span> s
                    w <span class="op">=</span> y <span class="op">-</span> tmid <span class="op">+</span> t
                    value <span class="op">+=</span> g[smid <span class="op">-</span> s, tmid <span class="op">-</span> t] <span class="op">*</span> f[v, w]
            h[x, y] <span class="op">=</span> value
    <span class="cf">return</span> h</code></pre></div>
<p>In normal python without Numba, this code takes 1.41s to run with a 100x100 and 8x8 array for f and g. With Numba and jit, this code takes 2.8 ms to execute, a speedup of more than 40x, but times depend on the machine used. It has been reported that Numba can speed code up up to 200x (<a href="https://en.wikipedia.org/wiki/Numba" class="uri">https://en.wikipedia.org/wiki/Numba</a>).</p>
<p>When we began looking into Numba, we were under the impression that there was an additional function called prange, which uses as many CPUs as detected by the multiprocessing module and runs loops in parallel. However, we then found out that this function had been removed and Numba no longer possesses parallelization options. At this point, we abandoned this strategy, but it is worth mentioning because of the incredible (non-parallel) speedup obtained for Python code with very minimal effort.</p>
<h1 id="reference" class="unnumbered">Reference</h1>
<div id="refs" class="references">
<div id="ref-Collaboration:2014eg">
<p>Collaboration TP, Ade PAR, Akiba Y et al (2014) A Measurement of the Cosmic Microwave Background B-Mode Polarization Power Spectrum at Sub-Degree Scales with POLARBEAR. arXivorg 171.</p>
</div>
<div id="ref-Hivon:2001eh">
<p>Hivon E, Gorski KM, Netterfield CB et al (2001) MASTER of the CMB Anisotropy Power Spectrum: A Fast Method for Statistical Analysis of Large and Complex CMB Data Sets. arXivorg 2–17.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>MPI is used to avoid the Python start-up overhead only. i.e. There is no communication between processes at all.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>trivial MPI refers to the use of MPI mentioned above that has no communication at all.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>typically Cython-tranpiled C++ code is on the order of <span class="math inline">\(100\)</span> times longer than the original Cython code.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Weave, formerly Scipy.weave, is a now deprecated tool to allow easy writing of C-modules in Python. Note that the Scipy team has migrated their own Weave codebase using Cython.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Cythonic is defined as oppose to Pythonic: Cythonic code is like writing C/C++ using Python syntax.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>except when creating and returning Numpy array as a Python object<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>which can be achieved by copying the data, <code>malloc</code>, and some pointer arithmetic, all available in Cython (but <code>aligned_alloc</code> is not).<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Physically, the <code>True</code> island represent the map we have for the CMB. Since we are scanning for a curvilinear sky, the map we made is not necessarily rectangular.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>in actual data size, it finishes on the order of <span class="math inline">\(\si{\ms}\)</span>.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>NERSC’s study has shown that Shifter can accelerate Python applications due to the huge metadata IO that Python needs. See more in <a href="http://www.nersc.gov/research-and-development/user-defined-images/">Shifter: User Defined Images</a> and <a href="https://www.nersc.gov/assets/Uploads/10-Python.pdf">An Introduction to Python at NERSC</a>.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>nemequ/icc-travis: Script to help install Intel C/C++ Compiler on Travis CI.<a href="#fnref11">↩</a></p></li>
</ol>
</section>
</body>
</html>
